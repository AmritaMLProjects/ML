{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49cdce9",
   "metadata": {},
   "source": [
    "#### Importing the Necessary Librarires and Ignoring the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0fa316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a56ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e261a",
   "metadata": {},
   "source": [
    "#### Read  and understand  the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da8dda6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'telecom_churn_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fj/4trbrrhn72b12j1r1vhm2g080000gn/T/ipykernel_72694/1712596316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'telecom_churn_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'telecom_churn_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('telecom_churn_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36351d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4378b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09848df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create backup of the original data frame\n",
    "original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72558f35",
   "metadata": {},
   "source": [
    "### Handling Missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d407641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking percent of missing values in columns\n",
    "df_missing_columns = (round(((df.isnull().sum()/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\n",
    "df_missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column name list by types of columns types\n",
    "id_cols = ['mobile_number', 'circle_id']\n",
    "\n",
    "date_cols = ['last_date_of_month_6',\n",
    "             'last_date_of_month_7',\n",
    "             'last_date_of_month_8',\n",
    "             'last_date_of_month_9',\n",
    "             'date_of_last_rech_6',\n",
    "             'date_of_last_rech_7',\n",
    "             'date_of_last_rech_8',\n",
    "             'date_of_last_rech_9',\n",
    "             'date_of_last_rech_data_6',\n",
    "             'date_of_last_rech_data_7',\n",
    "             'date_of_last_rech_data_8',\n",
    "             'date_of_last_rech_data_9'\n",
    "            ]\n",
    "\n",
    "cat_cols =  ['night_pck_user_6',\n",
    "             'night_pck_user_7',\n",
    "             'night_pck_user_8',\n",
    "             'night_pck_user_9',\n",
    "             'fb_user_6',\n",
    "             'fb_user_7',\n",
    "             'fb_user_8',\n",
    "             'fb_user_9'\n",
    "            ]\n",
    "\n",
    "num_cols = [column for column in df.columns if column not in id_cols + date_cols + cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e198938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of recharge columns where we will impute missing values with zeroes\n",
    "zero_impute = ['total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8', 'total_rech_data_9',\n",
    "        'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9',\n",
    "        'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'max_rech_data_9'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputint the missing values with 0\n",
    "df[zero_impute] = df[zero_impute].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id and date columns\n",
    "print(\"Shape before dropping: \", df.shape)\n",
    "churn_df = df.drop(id_cols + date_cols, axis=1)\n",
    "print(\"Shape after dropping: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with '-1' in categorical columns\n",
    "churn_df[cat_cols] = churn_df[cat_cols].apply(lambda x: x.fillna(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a99f41",
   "metadata": {},
   "source": [
    "#### Dropping columns with more than a given threshold of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da021279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=churn_df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking percent of missing values in columns\n",
    "# lets check the null values present in the dataset\n",
    "df=churn_df\n",
    "(df.isnull().sum() * 100 / len(df)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd32167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the null values present in the dataset\n",
    "(df.isnull().sum() * 100 / len(df)).sort_values(ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "(df.isnull().sum() * 100 / len(df)).sort_values(ascending = False).head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126df480",
   "metadata": {},
   "outputs": [],
   "source": [
    "inccol= [col for col in df.columns if df[col].isnull().any()]\n",
    "inccol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.median(),inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5292dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc18537",
   "metadata": {},
   "source": [
    "#### Find High value customers\n",
    "\n",
    "* To find the high value customers in the data - First we need to find the total recharge amount for the month\n",
    "\n",
    "Total Recharge Amount for month = (Number of Recharges) * (Average Recharge Amount)\n",
    "\n",
    "* Now we will calculate the total combined recharge amount for a month\n",
    "\n",
    "Total Combined recharge = Total Data Recharge Amount for month + Total Recharge Amount for month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total data recharge amount for June and July --> number of recharges * average recharge amount\n",
    "df['total_data_rech_6'] = df.total_rech_data_6 * df.av_rech_amt_data_6\n",
    "df['total_data_rech_7'] = df.total_rech_data_7 * df.av_rech_amt_data_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855be8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total recharge amount for June and July --> call recharge amount + data recharge amount\n",
    "df['amt_data_6'] = df.total_rech_amt_6 + df.total_data_rech_6\n",
    "df['amt_data_7'] = df.total_rech_amt_7 + df.total_data_rech_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average recharge done by customer in June and July\n",
    "df['av_amt_data_6_7'] = (df.amt_data_6 + df.amt_data_7)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the 70th percentile recharge amount\n",
    "print(\"Recharge amount at 70th percentile: {0}\".format(df.av_amt_data_6_7.quantile(0.7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only those customers who have recharged their mobiles with more than or equal to 70th percentile amount\n",
    "churn_df_filtered = df.loc[df.av_amt_data_6_7 >= df.av_amt_data_6_7.quantile(0.7), :]\n",
    "churn_df_filtered = churn_df_filtered.reset_index(drop=True)\n",
    "churn_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f743f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete variables created to filter high-value customers\n",
    "churn_df_filtered = churn_df_filtered.drop(['total_data_rech_6', 'total_data_rech_7',\n",
    "                                      'amt_data_6', 'amt_data_7', 'av_amt_data_6_7'], axis=1)\n",
    "churn_df_filtered.shape# delete variables created to filter high-value customers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa021dcf",
   "metadata": {},
   "source": [
    "### Derive churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd954de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total incoming and outgoing minutes of usage\n",
    "churn_df_filtered['total_calls_mou_9'] = churn_df_filtered.total_ic_mou_9 + churn_df_filtered.total_og_mou_9\n",
    "churn_df_filtered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_filtered['churn_df'] = churn_df_filtered.apply(lambda row: 1 if (row.total_calls_mou_9 == 0 ) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to category\n",
    "churn_df_filtered.churn_df = churn_df_filtered.churn_df.astype(\"category\")\n",
    "\n",
    "# print churn_df ratio\n",
    "print(\"churn_df Ratio's are:\")\n",
    "print(churn_df_filtered.churn_df.value_counts()*100/churn_df_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39485804",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_9_names = churn_df.filter(regex='9$', axis=1).columns\n",
    "\n",
    "# update num_cols and cat_cols column name list\n",
    "cat_cols = [col for col in cat_cols if col not in col_9_names]\n",
    "cat_cols.append('churn_df')\n",
    "num_cols = [col for col in churn_df_filtered.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will delete 9th month columns because we would predict churn_df/non-churn_df later based on data from the 1st 3 months\n",
    "cols_to_drop = [col for col in churn_df_filtered.columns if '_9' in col]\n",
    "print(cols_to_drop)\n",
    "\n",
    "churn_df_filtered.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "churn_df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037546",
   "metadata": {},
   "source": [
    "#### Dropping features with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c79146",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in churn_df_filtered.columns:\n",
    "    if churn_df_filtered[i].nunique() == 1:\n",
    "        print(\"\\nColumn\",i,\"has no variance and contains only\", churn_df_filtered[i].nunique(),\"unique value\")\n",
    "        print(\"Dropping the column\",i)\n",
    "        churn_df_filtered.drop(i,axis=1,inplace = True)\n",
    "\n",
    "print(\"\\nDimension of the updated dataset:\",churn_df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a236d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_filtered['arpu_diff'] = churn_df_filtered.arpu_8 - ((churn_df_filtered.arpu_6 + churn_df_filtered.arpu_7)/2)\n",
    "\n",
    "churn_df_filtered['onnet_mou_diff'] = churn_df_filtered.onnet_mou_8 - ((churn_df_filtered.onnet_mou_6 + churn_df_filtered.onnet_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['offnet_mou_diff'] = churn_df_filtered.offnet_mou_8 - ((churn_df_filtered.offnet_mou_6 + churn_df_filtered.offnet_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['roam_ic_mou_diff'] = churn_df_filtered.roam_ic_mou_8 - ((churn_df_filtered.roam_ic_mou_6 + churn_df_filtered.roam_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['roam_og_mou_diff'] = churn_df_filtered.roam_og_mou_8 - ((churn_df_filtered.roam_og_mou_6 + churn_df_filtered.roam_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['loc_og_mou_diff'] = churn_df_filtered.loc_og_mou_8 - ((churn_df_filtered.loc_og_mou_6 + churn_df_filtered.loc_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['std_og_mou_diff'] = churn_df_filtered.std_og_mou_8 - ((churn_df_filtered.std_og_mou_6 + churn_df_filtered.std_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['isd_og_mou_diff'] = churn_df_filtered.isd_og_mou_8 - ((churn_df_filtered.isd_og_mou_6 + churn_df_filtered.isd_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['spl_og_mou_diff'] = churn_df_filtered.spl_og_mou_8 - ((churn_df_filtered.spl_og_mou_6 + churn_df_filtered.spl_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['total_og_mou_diff'] = churn_df_filtered.total_og_mou_8 - ((churn_df_filtered.total_og_mou_6 + churn_df_filtered.total_og_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['loc_ic_mou_diff'] = churn_df_filtered.loc_ic_mou_8 - ((churn_df_filtered.loc_ic_mou_6 + churn_df_filtered.loc_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['std_ic_mou_diff'] = churn_df_filtered.std_ic_mou_8 - ((churn_df_filtered.std_ic_mou_6 + churn_df_filtered.std_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['isd_ic_mou_diff'] = churn_df_filtered.isd_ic_mou_8 - ((churn_df_filtered.isd_ic_mou_6 + churn_df_filtered.isd_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['spl_ic_mou_diff'] = churn_df_filtered.spl_ic_mou_8 - ((churn_df_filtered.spl_ic_mou_6 + churn_df_filtered.spl_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['total_ic_mou_diff'] = churn_df_filtered.total_ic_mou_8 - ((churn_df_filtered.total_ic_mou_6 + churn_df_filtered.total_ic_mou_7)/2)\n",
    "\n",
    "churn_df_filtered['total_rech_num_diff'] = churn_df_filtered.total_rech_num_8 - ((churn_df_filtered.total_rech_num_6 + churn_df_filtered.total_rech_num_7)/2)\n",
    "\n",
    "churn_df_filtered['total_rech_amt_diff'] = churn_df_filtered.total_rech_amt_8 - ((churn_df_filtered.total_rech_amt_6 + churn_df_filtered.total_rech_amt_7)/2)\n",
    "\n",
    "churn_df_filtered['max_rech_amt_diff'] = churn_df_filtered.max_rech_amt_8 - ((churn_df_filtered.max_rech_amt_6 + churn_df_filtered.max_rech_amt_7)/2)\n",
    "\n",
    "churn_df_filtered['total_rech_data_diff'] = churn_df_filtered.total_rech_data_8 - ((churn_df_filtered.total_rech_data_6 + churn_df_filtered.total_rech_data_7)/2)\n",
    "\n",
    "churn_df_filtered['max_rech_data_diff'] = churn_df_filtered.max_rech_data_8 - ((churn_df_filtered.max_rech_data_6 + churn_df_filtered.max_rech_data_7)/2)\n",
    "\n",
    "churn_df_filtered['av_rech_amt_data_diff'] = churn_df_filtered.av_rech_amt_data_8 - ((churn_df_filtered.av_rech_amt_data_6 + churn_df_filtered.av_rech_amt_data_7)/2)\n",
    "\n",
    "churn_df_filtered['vol_2g_mb_diff'] = churn_df_filtered.vol_2g_mb_8 - ((churn_df_filtered.vol_2g_mb_6 + churn_df_filtered.vol_2g_mb_7)/2)\n",
    "\n",
    "churn_df_filtered['vol_3g_mb_diff'] = churn_df_filtered.vol_3g_mb_8 - ((churn_df_filtered.vol_3g_mb_6 + churn_df_filtered.vol_3g_mb_7)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096616d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_filtered.churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c97677",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in churn_df_filtered.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b44c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_filtered[num_cols] = churn_df_filtered[num_cols].apply(pd.to_numeric)\n",
    "churn_df_filtered[cat_cols] = churn_df_filtered[cat_cols].apply(lambda column: column.astype(\"category\"), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e617c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plotting functions\n",
    "def data_type(variable):\n",
    "    if variable.dtype == np.int64 or variable.dtype == np.float64:\n",
    "        return 'numerical'\n",
    "    elif variable.dtype == 'category':\n",
    "        return 'categorical'\n",
    "    \n",
    "def univariate(variable, stats=True):\n",
    "    \n",
    "    if data_type(variable) == 'numerical':\n",
    "        sns.distplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.describe())\n",
    "    \n",
    "    elif data_type(variable) == 'categorical':\n",
    "        sns.countplot(variable)\n",
    "        if stats == True:\n",
    "            print(variable.value_counts())\n",
    "            \n",
    "    else:\n",
    "        print(\"Invalid variable passed: either pass a numeric variable or a categorical vairable.\")\n",
    "        \n",
    "def bivariate(var1, var2):\n",
    "    if data_type(var1) == 'numerical' and data_type(var2) == 'numerical':\n",
    "        sns.regplot(var1, var2)\n",
    "    elif (data_type(var1) == 'categorical' and data_type(var2) == 'numerical') or (data_type(var1) == 'numerical' and data_type(var2) == 'categorical'):        \n",
    "        sns.boxplot(var1, var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed72230",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(churn_df.arpu_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(churn_df.loc_og_t2o_mou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62642b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(churn_df.onnet_mou_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate(churn_df.offnet_mou_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4b6c4",
   "metadata": {},
   "source": [
    "#### From above we can infer that variables are skewed towards left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e26db",
   "metadata": {},
   "source": [
    "#### Bivariate EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654405cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate(churn_df_filtered.churn_df, churn_df_filtered.aon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate(churn_df_filtered.spl_og_mou_8, churn_df_filtered.churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec84a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(churn_df_filtered.churn_df, churn_df_filtered.night_pck_user_8, normalize='columns')*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7cc3ac",
   "metadata": {},
   "source": [
    "#### Cap outliers in all numeric variables with k-sigma technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4664bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(array, k=3):\n",
    "    upper_limit = array.mean() + k*array.std()\n",
    "    lower_limit = array.mean() - k*array.std()\n",
    "    array[array<lower_limit] = lower_limit\n",
    "    array[array>upper_limit] = upper_limit\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of capping\n",
    "sample_array = list(range(100))\n",
    "\n",
    "# add outliers to the data\n",
    "sample_array[0] = -9999\n",
    "sample_array[99] = 9999\n",
    "\n",
    "# cap outliers\n",
    "sample_array = np.array(sample_array)\n",
    "print(\"Array after capping outliers: \\n\", cap_outliers(sample_array, k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17054914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap outliers in the numeric columns\n",
    "churn_df_filtered[num_cols] = churn_df_filtered[num_cols].apply(cap_outliers, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fa515",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24315ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.metrics import sensitivity_specificity_support\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf20f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change churn_df to numeric\n",
    "churn_df_filtered['churn_df'] = pd.to_numeric(churn_df_filtered['churn_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938ba62",
   "metadata": {},
   "source": [
    "#### Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f03cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into train and test\n",
    "X = churn_df_filtered.drop(\"churn_df\", axis = 1)\n",
    "y = churn_df_filtered.churn_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fe87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of train and test sets\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e78017",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "# aggregate the categorical variables\n",
    "train.groupby('night_pck_user_6').churn_df.mean()\n",
    "train.groupby('night_pck_user_7').churn_df.mean()\n",
    "train.groupby('night_pck_user_8').churn_df.mean()\n",
    "train.groupby('fb_user_6').churn_df.mean()\n",
    "train.groupby('fb_user_7').churn_df.mean()\n",
    "train.groupby('fb_user_8').churn_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace categories with aggregated values in each categorical column\n",
    "mapping = {'night_pck_user_6' : {-1: 0.099165, 0: 0.066797, 1: 0.087838},\n",
    "           'night_pck_user_7' : {-1: 0.115746, 0: 0.055494, 1: 0.051282},\n",
    "           'night_pck_user_8' : {-1: 0.141108, 0: 0.029023, 1: 0.016194},\n",
    "           'fb_user_6'        : {-1: 0.099165, 0: 0.069460, 1: 0.067124},\n",
    "           'fb_user_7'        : {-1: 0.115746, 0: 0.059305, 1: 0.055082},\n",
    "           'fb_user_8'        : {-1: 0.141108, 0: 0.066887, 1: 0.024463}\n",
    "          }\n",
    "X_train.replace(mapping, inplace = True)\n",
    "X_test.replace(mapping, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data type of categorical columns - make sure they are numeric\n",
    "X_train[[col for col in cat_cols if col not in ['churn_df']]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pca to train data\n",
    "pca = Pipeline([('scaler', StandardScaler()), ('pca', PCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)\n",
    "churn_df_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pca model from pipeline\n",
    "pca = pca.named_steps['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(pca.n_components_)\n",
    "cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\n",
    "plt.figure(figsize=(175/20,100/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\n",
    "plt.plot(cumulative_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "PCA_VARS = 60\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         (\"pca\", PCA(n_components=PCA_VARS)),\n",
    "         (\"logistic\", LogisticRegression(class_weight='balanced'))\n",
    "        ]\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# check score on train data\n",
    "pipeline.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14a09e",
   "metadata": {},
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3066e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn_df on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f63ce6",
   "metadata": {},
   "source": [
    "#### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c499b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class imbalance\n",
    "y_train.value_counts()/y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA()\n",
    "\n",
    "# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "logistic = LogisticRegression(class_weight={0:0.1, 1: 0.9})\n",
    "\n",
    "# create pipeline\n",
    "steps = [(\"scaler\", StandardScaler()), \n",
    "         (\"pca\", pca),\n",
    "         (\"logistic\", logistic)\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "pca_logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'pca__n_components': [60, 80], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4253b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaa716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn_df on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc178ca",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\n",
    "forest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33333191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f61ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn_df on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609db50",
   "metadata": {},
   "source": [
    "#### From above we can infer that there is poor sensitivity. The best model is PCA along with Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4782bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a random forest model on train data\n",
    "max_features = int(round(np.sqrt(X_train.shape[1])))    # number of variables to consider to split each node\n",
    "print(max_features)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa91582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB score\n",
    "rf_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn_df on test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a735e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = churn_df_filtered.drop('churn_df', axis=1).columns\n",
    "\n",
    "# feature_importance\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "# create dataframe\n",
    "feature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\n",
    "feature_importance = feature_importance[['variables', 'importance_percentage']]\n",
    "\n",
    "# sort features\n",
    "feature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)\n",
    "print(\"Sum of importance=\", feature_importance.importance_percentage.sum())\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 'n' features\n",
    "top_n = 30\n",
    "top_features = feature_importance.variables[0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['total_ic_mou_8', 'total_rech_amt_diff', 'total_og_mou_8', 'arpu_8', 'roam_ic_mou_8', 'roam_og_mou_8', \n",
    "                'std_ic_mou_8', 'av_rech_amt_data_8', 'std_og_mou_8']\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee437c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "steps = [('scaler', StandardScaler()), \n",
    "         (\"logistic\", LogisticRegression(class_weight={0:0.1, 1:0.9}))\n",
    "        ]\n",
    "\n",
    "# compile pipeline\n",
    "logistic = Pipeline(steps)\n",
    "\n",
    "# hyperparameter space\n",
    "params = {'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n",
    "\n",
    "# create 5 folds\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# create gridsearch object\n",
    "model = GridSearchCV(estimator=logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44665492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ae719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best hyperparameters\n",
    "print(\"Best AUC: \", model.best_score_)\n",
    "print(\"Best hyperparameters: \", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict churn_df on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create onfusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# check sensitivity and specificity\n",
    "sensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\n",
    "print(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n",
    "\n",
    "# check area under curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49edb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = model.best_estimator_.named_steps['logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83524b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercept\n",
    "intercept_df = pd.DataFrame(logistic_model.intercept_.reshape((1,1)), columns = ['intercept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93be61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients\n",
    "coefficients = logistic_model.coef_.reshape((9, 1)).tolist()\n",
    "coefficients = [val for sublist in coefficients for val in sublist]\n",
    "coefficients = [round(coefficient, 3) for coefficient in coefficients]\n",
    "\n",
    "logistic_features = list(X_train.columns)\n",
    "coefficients_df = pd.DataFrame(logistic_model.coef_, columns=logistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate dataframes\n",
    "coefficients = pd.concat([intercept_df, coefficients_df], axis=1)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b01911",
   "metadata": {},
   "source": [
    "#### Business Insights\n",
    "* Telecom industry should take care of Roaming rates. They can provide offers to customers who are travelling internationally.\n",
    "* Also the telecom industry should provide discounts for the users who take long term packages like for 6 Months or 1 year. SO that more users are attracted.\n",
    "* They should also provide good customer support to there users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
